# Text analytics

[![MIT License](https://img.shields.io/apm/l/atomic-design-ui.svg?)](https://github.com/tterb/atomic-design-ui/blob/master/LICENSEs)
[![GPLv3 License](https://img.shields.io/badge/License-GPL%20v3-yellow.svg)](https://opensource.org/licenses/)
[![AGPL License](https://img.shields.io/badge/license-AGPL-blue.svg)](http://www.gnu.org/licenses/agpl-3.0)
[![author](https://img.shields.io/badge/author-RafaelGallo-red.svg)](https://github.com/RafaelGallo?tab=repositories) 
[![](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/release/python-374/) 
[![](https://img.shields.io/badge/R-3.6.0-red.svg)](https://www.r-project.org/)
[![](https://img.shields.io/badge/ggplot2-white.svg)](https://ggplot2.tidyverse.org/)
[![](https://img.shields.io/badge/dplyr-blue.svg)](https://dplyr.tidyverse.org/)
[![](https://img.shields.io/badge/readr-green.svg)](https://readr.tidyverse.org/)
[![](https://img.shields.io/badge/ggvis-black.svg)](https://ggvis.tidyverse.org/)
[![](https://img.shields.io/badge/Shiny-red.svg)](https://shiny.tidyverse.org/)
[![](https://img.shields.io/badge/plotly-green.svg)](https://plotly.com/)
[![](https://img.shields.io/badge/XGBoost-red.svg)](https://xgboost.readthedocs.io/en/stable/#)
[![](https://img.shields.io/badge/Tensorflow-orange.svg)](https://powerbi.microsoft.com/pt-br/)
[![](https://img.shields.io/badge/Keras-red.svg)](https://powerbi.microsoft.com/pt-br/)
[![](https://img.shields.io/badge/CUDA-gree.svg)](https://powerbi.microsoft.com/pt-br/)
[![](https://img.shields.io/badge/Caret-orange.svg)](https://caret.tidyverse.org/)
[![](https://img.shields.io/badge/Pandas-blue.svg)](https://pandas.pydata.org/) 
[![](https://img.shields.io/badge/Matplotlib-blue.svg)](https://matplotlib.org/)
[![](https://img.shields.io/badge/Seaborn-green.svg)](https://seaborn.pydata.org/)
[![](https://img.shields.io/badge/Matplotlib-orange.svg)](https://scikit-learn.org/stable/) 
[![](https://img.shields.io/badge/Scikit_Learn-green.svg)](https://scikit-learn.org/stable/)
[![](https://img.shields.io/badge/Numpy-white.svg)](https://numpy.org/)
[![](https://img.shields.io/badge/PowerBI-red.svg)](https://powerbi.microsoft.com/pt-br/)

![Logo](https://img.freepik.com/free-vector/gradient-stock-market-concept_23-2149166929.jpg?w=1380&t=st=1690864999~exp=1690865599~hmac=9560af55f1bc856f1f8f0c34bc9923f5edd97ee1beffa21ddb37361180c50229)

### Descrição do projeto 

## Parte 1 - Processamento de linguagem natural (NLP)

O projeto de análise de texto com processamento de linguagem natural (NLP) é uma iniciativa inovadora que tem como objetivo explorar e utilizar avançadas técnicas de NLP para compreender, interpretar e extrair informações valiosas a partir de textos diversos. Com a crescente quantidade de dados não estruturados disponíveis na internet, a capacidade de analisar e tirar insights significativos desses dados se tornou crucial em diversos setores, incluindo pesquisa, negócios, saúde, entretenimento e muito mais. Neste projeto, utilizamos as mais recentes ferramentas e algoritmos de processamento de linguagem natural para processar textos em diferentes idiomas, incluindo análise de sentimento, classificação de tópicos, extração de entidades, resumo automático e tradução automática. Além disso, buscamos aperfeiçoar a capacidade de nossa análise em lidar com contextos complexos e ambíguos, o que é especialmente desafiador em textos informais e linguagem coloquial.

Um dos principais objetivos desse projeto é a criação de um modelo de aprendizado de máquina altamente eficiente, capaz de lidar com grandes volumes de texto em tempo real, permitindo a análise em escala de dados provenientes de redes sociais, blogs, artigos científicos, entre outras fontes. Isso possibilitará que empresas e pesquisadores tomem decisões mais informadas e estratégicas com base em dados textuais relevantes e precisos. Ao longo do desenvolvimento do projeto, também pretendemos explorar a ética e a privacidade relacionadas à análise de texto, garantindo que todas as práticas estejam em conformidade com as leis e regulamentos vigentes, além de respeitar os direitos individuais dos usuários e a confidencialidade das informações. 

Com a conclusão deste projeto de análise de texto com processamento de linguagem natural (NLP), esperamos contribuir significativamente para o avanço da área, tornando a tecnologia mais acessível e benéfica para a sociedade como um todo. Nossos resultados e avanços serão compartilhados por meio de publicações científicas e disponibilizados como uma ferramenta acessível para que outros pesquisadores e desenvolvedores possam se beneficiar e expandir ainda mais os horizontes do processamento de linguagem natural. Em resumo, o projeto de análise de texto com processamento de linguagem natural é uma iniciativa ambiciosa que busca transformar a maneira como interagimos com a vasta quantidade de dados textuais disponíveis, proporcionando insights valiosos e melhorando a compreensão humana sobre a linguagem escrita em suas diversas formas.

## Parte 2 - Modelo ARIMA SARIMA para previsão das ações

A análise de séries temporais com os modelos ARIMA (Autoregressive Integrated Moving Average) e SARIMA (Seasonal Autoregressive Integrated Moving Average) para realizar previsões das ações. Com o crescimento exponencial da quantidade de dados não estruturados disponíveis na internet, a necessidade de compreender e extrair informações valiosas a partir desses textos se tornou crucial. Nesse sentido, a utilização de técnicas avançadas de processamento de linguagem natural nos permite explorar e interpretar esses dados, identificando padrões e tendências ocultas que podem fornecer insights valiosos para o mercado financeiro. Além disso, o uso dos modelos ARIMA e SARIMA na análise de séries temporais é uma abordagem estatística poderosa para prever o comportamento futuro das ações. 

Esses modelos levam em consideração a estrutura temporal dos dados, incluindo tendências e sazonalidades, e são amplamente utilizados em previsões financeiras devido à sua eficácia comprovada. Com a combinação dessas duas abordagens - análise de texto com processamento de linguagem natural e análise de séries temporais com ARIMA e SARIMA - buscamos criar um sistema abrangente que seja capaz de realizar previsões mais precisas e informadas sobre o mercado de ações. Essas previsões podem ser extremamente úteis para investidores, gestores de fundos, analistas financeiros e outros profissionais do mercado, auxiliando-os na tomada de decisões estratégicas.

Durante o desenvolvimento do projeto, daremos ênfase à seleção adequada dos dados de texto e séries temporais, bem como ao treinamento e ajuste dos modelos ARIMA e SARIMA para garantir a eficiência e a precisão das previsões. Além disso, realizaremos uma análise rigorosa dos resultados obtidos, avaliando a qualidade das previsões e identificando possíveis desafios e limitações para futuras melhorias. 

Em suma, este projeto representa um passo importante em direção ao avanço da análise de dados financeiros, combinando as poderosas técnicas de processamento de linguagem natural com a eficácia dos modelos ARIMA e SARIMA. Esperamos que nossas contribuições resultem em previsões mais confiáveis e úteis para o mercado de ações, beneficiando tanto investidores individuais quanto institucionais em suas estratégias de investimento e tomada de decisões.

## Parte 3 - Extração de tópicos utilizando modelos LDA e BART Topics e transformers dos textos de notícias com visualização dos resultados em gráficos de nuvem.

Na terceira fase deste projeto, focaremos na extração de tópicos a partir dos textos de notícias utilizando dois modelos avançados: LDA (Latent Dirichlet Allocation) e BART Topics. Esses modelos de processamento de linguagem natural são amplamente reconhecidos por sua eficácia na identificação e agrupamento de tópicos latentes presentes nos documentos.

O modelo LDA é uma abordagem estatística que considera que cada documento é uma mistura de tópicos e cada tópico é uma mistura de palavras. Ao aplicarmos o LDA aos textos de notícias, poderemos identificar os principais temas discutidos nos documentos e entender como eles estão distribuídos ao longo do corpus.

Por outro lado, o BART Topics é uma variação avançada do modelo LDA, que utiliza técnicas de aprendizado profundo e transformers para melhorar ainda mais a precisão na identificação dos tópicos. Essa abordagem permite uma análise mais refinada e sutil dos tópicos, considerando nuances e contextos específicos presentes nos documentos. Após a aplicação dos modelos LDA e BART Topics nos textos de notícias, realizaremos a visualização dos resultados por meio de gráficos de nuvem. Essa forma de representação gráfica é extremamente útil para proporcionar uma visão geral dos tópicos mais relevantes e suas frequências de ocorrência. As palavras mais importantes de cada tópico serão destacadas em tamanho e intensidade, facilitando a interpretação e compreensão dos resultados.

Ao combinar a extração de tópicos com a visualização em gráficos de nuvem, seremos capazes de identificar rapidamente os assuntos mais discutidos nas notícias, as tendências emergentes e as correlações entre diferentes tópicos. Essas informações serão de grande valor para pesquisadores, analistas de mercado, jornalistas e outros profissionais que dependem de insights precisos e atualizados para suas atividades. Ademais, a análise de tópicos também pode ser aplicada em áreas como monitoramento de mídias sociais, pesquisa de opinião pública, análise de feedback do cliente e muito mais. A capacidade de extrair automaticamente tópicos relevantes de grandes volumes de texto é uma vantagem competitiva significativa em diversas áreas. 

Nesta fase do projeto, também dedicaremos atenção à interpretação dos resultados e à validação dos tópicos extraídos. É essencial garantir que os tópicos sejam coerentes, interpretáveis e relevantes para os objetivos específicos do estudo. Em resumo, a Parte 3 deste projeto concentra-se na aplicação dos modelos LDA e BART Topics para extrair tópicos significativos dos textos de notícias e na representação visual desses resultados através de gráficos de nuvem. A análise de tópicos é uma etapa crucial para obter insights relevantes e orientar tomadas de decisão informadas em diversas áreas. Com esse avanço, esperamos contribuir para o aprimoramento das técnicas de processamento de linguagem natural e fornecer uma ferramenta valiosa para a análise e interpretação de grandes volumes de texto em diferentes contextos.

## Autores

- [@RafaelGallo](https://github.com/RafaelGallo)


## Licença

[MIT](https://choosealicense.com/licenses/mit/)


## Resultados - Dos modelos machine learning 

- Melhorar o suporte de navegadores

- Adicionar mais integrações


## Variáveis de Ambiente

Para rodar esse projeto, você vai precisar adicionar as seguintes variáveis de ambiente no seu .env

`API_KEY`

`ANOTHER_API_KEY`
## Instalação 

```bash
  conda install pandas 
  conda install scikitlearn
  conda install numpy
  conda install scipy
  conda install matplotlib
  conda install keras
  conda install tensorflow-gpu==2.5.0

  python==3.6.4
  numpy==1.13.3
  scipy==1.0.0
  matplotlib==2.1.2
```
Instalação do Python É altamente recomendável usar o anaconda para instalar o python. Clique aqui para ir para a página de download do Anaconda https://www.anaconda.com/download. Certifique-se de baixar a versão Python 3.6. Se você estiver em uma máquina Windows: Abra o executável após a conclusão do download e siga as instruções. 

Assim que a instalação for concluída, abra o prompt do Anaconda no menu iniciar. Isso abrirá um terminal com o python ativado. Se você estiver em uma máquina Linux: Abra um terminal e navegue até o diretório onde o Anaconda foi baixado. 
Altere a permissão para o arquivo baixado para que ele possa ser executado. Portanto, se o nome do arquivo baixado for Anaconda3-5.1.0-Linux-x86_64.sh, use o seguinte comando: chmod a x Anaconda3-5.1.0-Linux-x86_64.sh.

Agora execute o script de instalação usando.


Depois de instalar o python, crie um novo ambiente python com todos os requisitos usando o seguinte comando

```bash
conda env create -f environment.yml
```
Após a configuração do novo ambiente, ative-o usando (windows)
```bash
activate "Nome do projeto"
```
ou se você estiver em uma máquina Linux
```bash
source "Nome do projeto" 
```
Agora que temos nosso ambiente Python todo configurado, podemos começar a trabalhar nas atribuições. Para fazer isso, navegue até o diretório onde as atribuições foram instaladas e inicie o notebook jupyter a partir do terminal usando o comando
```bash
jupyter notebook
```
## Stack utilizada

**Machine learning:** Python, R

**Framework:** Scikit-learn

**Análise de dados:** Python, R

## Base de dados - Modelos de machine learning

| Dataset               | Link                                                 |
| ----------------- | ---------------------------------------------------------------- |
|Tesla Latest News | https://www.kaggle.com/datasets/ilyaryabov/tesla-latest-news|
| Elon Musk's Tweets Dataset 2022 | https://www.kaggle.com/datasets/marta99/elon-musks-tweets-dataset-2022?select=rawdata.csv|

## Variáveis de Ambiente

Para rodar esse projeto, você vai precisar adicionar as seguintes variáveis de ambiente no seu .env

Instalando a virtualenv

`pip install virtualenv`

Nova virtualenv

`virtualenv nome_virtualenv`

Ativando a virtualenv

`source nome_virtualenv/bin/activate` (Linux ou macOS)

`nome_virtualenv/Scripts/Activate` (Windows)

Retorno da env

`projeto_py source venv/bin/activate` 

Desativando a virtualenv

`(venv) deactivate` 

Instalando pacotes

`(venv) projeto_py pip install flask`

Instalando as bibliotecas

`pip freeze`

## Uso/Exemplos - Modelo machine learning

# Instalação

Instalação das bibliotecas para esse projeto no python.

```bash
  conda install pandas 
  conda install scikitlearn
  conda install numpy
  conda install scipy
  conda install matplotlib
  conda install nltk

  python==3.6.4
  numpy==1.13.3
  scipy==1.0.0
  matplotlib==2.1.2
  nltk==3.6.7
```
Instalação do Python É altamente recomendável usar o anaconda para instalar o python. Clique aqui para ir para a página de download do Anaconda https://www.anaconda.com/download. Certifique-se de baixar a versão Python 3.6. Se você estiver em uma máquina Windows: Abra o executável após a conclusão do download e siga as instruções. 

Assim que a instalação for concluída, abra o prompt do Anaconda no menu iniciar. Isso abrirá um terminal com o python ativado. Se você estiver em uma máquina Linux: Abra um terminal e navegue até o diretório onde o Anaconda foi baixado. 
Altere a permissão para o arquivo baixado para que ele possa ser executado. Portanto, se o nome do arquivo baixado for Anaconda3-5.1.0-Linux-x86_64.sh, use o seguinte comando: chmod a x Anaconda3-5.1.0-Linux-x86_64.sh.

Agora execute o script de instalação usando.


Depois de instalar o python, crie um novo ambiente python com todos os requisitos usando o seguinte comando

```bash
conda env create -f environment.yml
```
Após a configuração do novo ambiente, ative-o usando (windows)
```bash
activate "Nome do projeto"
```
ou se você estiver em uma máquina Linux
```bash
source "Nome do projeto" 
```
Agora que temos nosso ambiente Python todo configurado, podemos começar a trabalhar nas atribuições. Para fazer isso, navegue até o diretório onde as atribuições foram instaladas e inicie o notebook jupyter a partir do terminal usando o comando
```bash
jupyter notebook
```
    
## Exemplo Modelo LDA

```
## Python
# Importação das bibliotecas
import gensim
from gensim import corpora
from pprint import pprint

# Exemplo de corpus de notícias fictício (lista de documentos)
corpus = [
    "A Apple lançou um novo iPhone com tecnologia avançada.",
    "As ações da empresa Google tiveram um aumento expressivo no mercado.",
    "O mercado financeiro reagiu positivamente ao anúncio da nova política econômica.",
    "As vendas da empresa XYZ atingiram um recorde no último trimestre.",
    "A economia está se recuperando após a crise financeira global."
]

# Pré-processamento dos documentos
# Separar as palavras, converter em minúsculas e remover stopwords (palavras comuns sem relevância)
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))

def preprocess(text):
    words = word_tokenize(text.lower())
    return [word for word in words if word.isalpha() and word not in stop_words]

processed_corpus = [preprocess(doc) for doc in corpus]

# Criação do dicionário a partir do corpus
dictionary = corpora.Dictionary(processed_corpus)

# Criação do corpus vetorizado
bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]

# Treinamento do modelo LDA
num_topics = 2  # Número de tópicos a serem extraídos
lda_model = gensim.models.LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10)

# Resultados
print("Tópicos encontrados:")
pprint(lda_model.print_topics())

## Modelo BERT em R
# Instalar os pacotes necessários
install.packages("huggingface")

# Carregar as bibliotecas
library(huggingface)

# Carregar o modelo pré-treinado BERT
model <- Huggingface_Model("bert-base-uncased")

# Texto de exemplo
texto <- "Eu adoro usar o modelo BERT para tarefas de processamento de linguagem natural."

# Tokenizar o texto
tokens <- model$tokenize(texto)

# Codificar os tokens
input_ids <- model$encode(tokens)

# Realizar a predição do modelo
output <- model$predict(input_ids)

# Imprimir os resultados
print(output)
```

## Feedback
Se você tiver algum feedback, por favor nos deixe saber por meio de rafaelhenriquegallo@gmail.com.br

## Melhorias
Que melhorias você fez no seu código? Ex: refatorações, melhorias de performance, acessibilidade, etc


## Referência

 - [Awesome Readme Templates](https://awesomeopensource.com/project/elangosundar/awesome-README-templates)
 - [Awesome README](https://github.com/matiassingers/awesome-readme)
 - [How to write a Good readme](https://bulldogjob.com/news/449-how-to-write-a-good-readme-for-your-github-project)


## Licença

[MIT](https://choosealicense.com/licenses/mit/)


## Suporte

Para suporte, mande um email para rafaelhenriquegallo@gmail.com
